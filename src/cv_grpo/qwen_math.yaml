# Terminal code
# ACCELERATE_LOG_LEVEL=info \
#     accelerate launch --config_file src/cv_grpo/zero3.yaml \
#     src/open_r1/grpo_math_simple.py --config src/cv_grpo/qwen_math.yaml \
#     --vllm_mode colocate

# Model arguments (Qwen/Qwen2.5-1.5B-Instruct / Qwen/Qwen2.5-3B-Instruct)
model_name_or_path: Qwen/Qwen2.5-Math-1.5B
model_revision: main
torch_dtype: bfloat16
attn_implementation: flash_attention_2

# Data training arguments
dataset_name: jhn9803/hendrycks-math-with-answers

# Combinations of methods and loss types:
# GRPO : grpo-grpo
# DR.GRPO : dr_grpo-dr_grpo
# RLOO : rloo-grpo
# CV.GRPO : cv_grpo-grpo

# Method arguments (grpo, dr_grpo, rloo, cv_grpo)
method: rloo
scale_rewards: false
# loss_type : grpo, bnpo, dr_grpo
loss_type: reinforce
tau: 0.3

# NAME designation
push_to_hub: true
hub_model_id: Qwen2.5-MATH-1.5B-BASE-RLOO-EP3-LR2e06

output_dir: /ext_hdd/jhna/rebuttal_cvapo/Qwen2.5-MATH-1.5B-BASE-RLOO-EP3-LR2e06
run_name: Qwen2.5-MATH-1.5B-BASE-RLOO-EP3-LR2e06
wandb_entity: howdydoo
project_name: rebuttal_cvapo

# GRPO trainer config
bf16: true
use_vllm: true
vllm_gpu_memory_utilization: 0.4
vllm_engine_args:
  max_model_len: 4096
gradient_accumulation_steps: 4
gradient_checkpointing: true
gradient_checkpointing_kwargs:
  use_reentrant: false
hub_strategy: every_save
learning_rate: 2.0e-06
log_completions: true
log_level: info
logging_first_step: true
logging_steps: 1
logging_strategy: steps
max_prompt_length: 1024
max_completion_length: 1024
max_steps: -1
num_generations: 8
num_train_epochs: 3
epsilon: 0.1
beta: 0.0

lr_scheduler_type: cosine
warmup_ratio: 0.0

overwrite_output_dir: true
per_device_train_batch_size: 16

do_eval: false
eval_strategy: "no"
per_device_eval_batch_size: 16

reward_funcs:
- drgrpo_math

report_to:
- wandb

save_strategy: "epoch"
save_total_limit: 3
seed: 42

# use_peft: true
# peft_type: lora
# lora_r: 8
# lora_alpha: 16
# lora_dropout: 0.05
# lora_target_modules:
# - q_proj
# - k_proj
# - v_proj
# - o_proj

